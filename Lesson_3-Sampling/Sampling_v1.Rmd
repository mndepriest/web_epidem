---
title: "Sampling"
author: "Mladen Cucak; Felipe Dalla Lana; Mauricio Serrano; Paul Esker, Miranda DePriest"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float:
      toc_depth: 3
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


# **Introduction** 

Sampling, or determining which individuals to collect data from, is an important aspect of designing and conducting an experiment. Sampling is often necessary because collecting data from a whole population can be impossible. The method of sampling you chose can determine the quality of your data, the statistical analyses you can use, and impact the reliability of any insights from your experiment. The sampling method employed can often depend on the size and structure of the population, the resources you have, and the type of questions you are asking. 

In this lesson, we will examine many of the most common sampling methods. In each section, you will learn about how to employ the sampling method and when to use it. We will also discuss how to assess the reliability of the results we get when sampling.


### **Setting up**

First, we need to set up our R session. In the next section of code, we will load the packages we need for this lesson. If you encounter difficulty, please see Lesson 1 - Intro to R. 

```{r libs, message=FALSE, warning=FALSE,class.source = 'fold-show'}

# First, we will set a CRAN 'mirror'. This directs R to install the following packages from a certain CRAN repository ('repo'). Normally, R automatically sets the CRAN repo, but sometimes using .Rmd files (instead of .R files) can cause problems.
local({r <- getOption("repos")
       r["CRAN"] <- "https://cran.r-project.org" 
       options(repos=r)
})


# Instead of doing this:
# library("dplyr")
# library("tidyr")
# library("ggplot2")
# ...
# ...

# We will create a list of desired packages and pass it to the `library()` command. 

# Make the list of desired packages
list.of.packages <-
  c("tidyverse", # Remember, tidyverse contains several packages, including ggplot2
    "conflicted", 
    "here", 
    "plotly", 
    "emdbook", 
    "ROCR"
  )

# Make a list of any packages that aren't installed
new.packages <-
  list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]

# If the list of un-installed packages has any packages listed, install them
if (length(new.packages))
  install.packages(new.packages)

# Load all packages into this R session
packages_load <-
  lapply(list.of.packages, require, character.only = TRUE)

# Print a warning if there is a problem with installing/loading some of packages
if (any(as.numeric(packages_load) == 0)) {
  warning(paste("Package/s: ", paste(list.of.packages[packages_load != TRUE], sep = ", "), "not loaded!"))
} else {
  print("All packages were successfully loaded.")
}

# Remove the package-related objects we created
rm(list.of.packages, new.packages, packages_load)

# Resolve conflicts (identical commands with different functions) between packages. 
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("layout", "plotly")

# If installation is not working, try changing repos. CRAN repos can be found here: https://cran.r-project.org/mirrors.html 

```


# **Evaluating sampling reliability**

Sample data is used to estimate the characteristics of the entire population. In plant pathology, you could evaluate the disease severity of a sample of plants to predict how the whole population of plants will respond. Because the whole population could not be surveyed, there is **inherent uncertainty**. How can we improve the *reliability* of the result? How can we predict *how much sampling* (the reliable sample size) is needed to create a reliable result?  

> **Reliability**: A result can be considered reliable when it can be replicated. In other words, this is when random re-sampling produces similar results.


### **The coefficient of variation (CV)**

The reliability of a result can depend on the *variation* between observations. In our case, this could be the differences in disease severity among sampled plants. The spread of these values can determine how much sampling is necessary to form a reliable result. Variation in disease severity among plants can come from intentional experimental factors, like treatment, or from random events, such as the differences between individuals. 

To measure variability, we can calculate the **coefficient of variation** (CV). The CV can be calculated by:

CV = σ / μ

where σ is the standard deviation in the data and μ is the mean. Essentially, we can interpret the CV as the ratio between the standard deviation and the mean. 

**Note:** The standard deviation is equivalent to the square root of the variance. 

**Example 1.1: Calculating the CV of a simple data set**

In this example, we survey 50 plants for the presence or absence of disease. 

```{r cv_calc,class.source = 'fold-show'}

# First, we create the example data:

# Total number of plants
n_plants <- 50 

# Number of diseased plants
infected <- 20 


# Now, we'll calculate the CV of the data

# The incidence (mean) of disease
inc <- infected/n_plants
inc

# We calculate the variance in disease incidence
var <- (inc* (1 - inc))/n_plants
cv
# Note - R has a function for calculating variance, but it's important to see how it is calculated. 

# We use the standard deviation (square root of the variance) and the mean to calculate the CV
cv <- sqrt(var)/inc
cv

```

In this example data set, we got a CV of 0.173 (17.3%). In plant pathology research, values between 0.1 and 0.2 (10% to 20%) are typically considered satisfactory.

### **The index of dispersion**

The index of dispersion (ID) is related to the CV. It is a ratio determined by the variance and mean. The distribution pattern of the data is important when interpreting the ID. In data that has a binomial distribution, an ID of less than 1 is considered ideal. 

The index of dispersion can be calculated by:

ID = var^2^/mean

We can use the previous example to show the calculation of an ID:

```{r id_calc,class.source = 'fold-show'}

# Use the variance (var) and mean (inc) to calculate the ID
id <- (var^2)/inc

```

### **Confidence interval half-width (CI half-width)**

We can also use the range of the confidence interval as an indicator of reliability. 

Confidence intervals (CIs) represent the probability that a random observation will fall between two points. Just like the CV, CIs depend on data standard deviation and mean. Typically, we will use a CI that encompasses at least 95% of the data. This interval will contain values that differ by less than 1.96 standard deviations from the mean. The half width of this interval will be equivalent to the standard deviation multiplied by 1.96. 

CI half width (95%) = 1.96 * var^1/2^

**Example 1.2: Calculating the half width of the 95% CI**

We will use the previous example data to calculate the CI half-width. For this example, we will calculate the half-width of the CI that encompasses 95% of the data. Results that fall within half of this width are considered reliable. 

```{r half_width_calc,class.source = 'fold-show'}

# Half-width of the required confidence interval
1.96*sqrt(var) 


```

This means that for the incidence rate of 0.4 (40%) to be considered a reliable result, results from random re-sampling must fall within 0.4 $\pm$ 0.135 (13.5%). 


### **Finite population correction factor (FPC)**

We must also consider the impact of sampling a relatively large portion of the population without replacing specimens back into the sampling pool. As specimens are sampled and 'removed' from the pool of available specimens to sample, the remaining population changes in composition. Typically, finite population correction is necessary when sampling 10% or more of the total population. 

The finite population correction *factor* (FPC) is calculated by:

FPC = ((N-n)/(N-1))^(1/2)^

where N represents the total population and n represents the sampled population. The FPC is bounded between 0 and 1, and decreases as sampling efforts impact the remaining population. Typically, if the FPC is below 0.95 (95%), the size of the sampled individuals needs to be addressed. 


**Example 2: Calculating the FPC by sampling effort in a population of 10,000**

In the next example, we are going to see how the number of individuals sampled impacts the FPC. The example population includes 10,000 individuals.  

```{r fpc_calc}

# Create the example data
df <- data.frame(n = seq(0, 10000, by = 500),
                 N = 10000) %>%
  mutate(prop = (n/N)*100, # This represents the proportion of the population sampled
         fpc = ((N-n)/(N-1))^(1/2))
df


ggplot(df, aes(prop, fpc)) +
  geom_point() +
  xlab("Proportion of surveyed individuals (%)") +
  ylab("FPC")


```


In the graph above, we can see that at smaller sampling ranges, the FPC remains close to 1. This means that the impact of sampling without replacement on the remaining population is negligible. However, the FPC drops below 0.95 when more than 10% of the population is being sampled without replacement. There are a number of methods to use if this becomes this case, such as sampling *with* replacement or by simply reducing your sampling effort. The choice of method will depend on the situation.


# **Sampling methods** 

Now we will discuss common sampling methods, particularly randomized sampling and clustered/blocked sampling. We will also provide examples for when the use of a certain method is appropriate. The sampling method you choose to use is important in reducing elements that can bias your findings. 

Some things that can bias the results of a study are:
> Sampling bias: This is when certain individuals are selected for data collection according to the researcher's conscious or unconscious bias. 

Sampling bias example 1 - A researcher chooses not to select individuals from a part of a field that is hard to get to. Random sampling can ensure that individuals from this part of the field are as likely to be collected as any other individual in the experiment.

Sampling bias example 2: Although the researcher does not know it, a certain proportion of the field has been subjected to different environmental conditions. Random sampling improves the chances of collecting a representative number of individuals from that area of the field compared to the rest of the field. This way, the results of the experiment are less likely to attribute any variation in plants to the treatment when in reality they were due to the different field conditions. 

### **Simple random sampling (SRS)**

Simple random sampling (SRS) is the most basic way to implement randomness in a sampling plan. Individuals at selected at random for collecting data, removing things that can introduce bias. 

For the following example, please imagine that we are sampling to determine what percent of the population has disease, with the true disease incidence being 41%. 

**Example 3: SRS for accurately sampling disease severity ** 

In this example, we will sample randomly without replacement. This is because, as individuals are surveyed and removed from the population, the population doesn't not change significantly. We can use base R to obtain a list of randomly selected plants to sample. Then, we will find our resulting disease incidence and use the CV to see if our results are reliable. 


```{r srs, class.source = 'fold-show'}

# Let's create our example data, where 0 is no disease and 1 is disease
pop <- data.frame(plant_id = 1:1000) %>%
  mutate(status = sample(c(0, 1), n(),
                        replace = TRUE, prob = c((1-0.41), 0.41)))
head(pop)

# We will use `set.seed()` to generate a pseudorandom number so that the class gets the same results. This is a great way to ensure that your work is reproducible while still getting randomly selected numbers later. The number you pick isn't important, but recording it is! 
set.seed(1866844)

# Use base R function `sample()` to pick specimens without replacement
sampled.plants <- sample(pop$plant_id, 25, replace = FALSE)

# Create a data frame of only the sampled plants
sampled.df <- pop[sampled.plants,]

# Calculate the incidence of disease
inc <- (sum(sampled.df$status)/25)
inc

# Calculate the variance
var <- (inc * (1-inc))/nrow(sampled.df)

# Calculate the CV 
cv <- sqrt(var)/inc


```


We got a disease incidence rate of 52%, which is a bit off compared to the true value of 41%. The CV was within 20%, so it may be considered satisfactory. But what if this research was in a field where 20% wasn't considered satisfactory, and we had to plan how many samples to take to get a CV of approximately 10% or less? 

We can use the following formula to estimate the number of samples we'll need for a CV of 10% or less:

n = (1-inc)/(inc*0.1^2^)

Let's see approximately how many samples we would need. 


```{r srs_nsamples, class.source = 'fold-show'}

# Calculate the number of samples needed
n <- (1-inc)/(inc*0.1^2)

```


We would need approx. 92 plants to get a CV of 10% or less. 


### **Cluster sampling**

Cluster sampling is a method of sampling where sub-sections ("clusters") of the whole population are designated for sampling. This method can be appropriate in instances where the population is divided, such as when fields are split into quadrants, or when an experiment takes place over multiple fields. Clusters are usually determined randomly, but in some scenarios, researchers must choose clusters based on elements like accessibility. Randomness can also be used when sampling *within* a cluster by using a simple random sampling technique to select individuals from a cluster. 

Cluster sampling can be great when visiting each location in an experiment is costly, but the validity of the results are dependent on those clusters accurately representing the population. For plant pathology research, this often means that every treatment must be represented in every cluster. 

**Example 4: Cluster sampling for disease severity**

In this example, we will imagine that we have an experiment consisting of 300 quadrants (20 x 15), with 10 plants each. The true value for the incidence of disease is 30%. We are interested in determining the incidence of disease, but we can't sample every quadrant. 

We will visualize the disease incidence in each quadrant by building a matrix in R. Then, we will randomly sample 10 quadrants and evaluate our results. 

```{r cluster_sampling, class.source = 'fold-show'}

# Use set.seed() for reproducibility
set.seed(8486)

# Create normally distributed data showing how many plants are diseased in each quadrant
field <- rbinom(n = 300, size = 10, prob = 0.30)

# Provide the column and row of each quadrant in a data frame with the disease incidence
field <- data.frame(rows = rep(1:20, times = 15),
                    cols = rep(1:15, each = 20),
                    n.dis = field)

# Now we can plot our quadrants and show the number of plants with disease in each
ggplot(field, aes(x = cols, y = rows, color = n.dis, 
                  label = as.character(n.dis))) +
  geom_point(size = 10, shape = 15) +
  geom_text(colour = "black") +
  scale_color_gradient("Number of infected plants",
                       low = "lightgreen", high = "orangered") +
  theme_bw() + 
  theme(axis.title = element_blank(),
        legend.position = "top", 
        panel.background = element_rect(terrain.colors(5)[3]),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_equal()

# Set seed 
# set.seed(468468)
set.seed(46846811)

# Randomly pick 8 quadrants to sample
sampled.plants <- sample(300, 10, replace = FALSE)

# Create a data frame with only sampled plants
sampled.df <- field[sampled.plants,]

# Get sampled disease incidence rate 
inc <- sum(sampled.df$n.dis)/(10*10)

# Calculate the overall variance
var <- (inc * (1-inc))/(10*10)

# Calculate the CV 
cv <- sqrt(var)/inc

# Calculate the number of quadrants needed for CV of 10%
n <- (1-inc)/(10*inc*0.2^2)

# Calculate the ID

```


## **Cluster sampling for disease incidence**

This code, when copied and pasted, will duplicate the information needed to replicate the example in the notes pertaining to cluster sampling for disease incidence data:

-   Number of quadrants = 300
-   Probability of success (i.e., disease) = 0.35
-   theta \$\theta = overdispersion parameter, which is connected to shape1 and shape2 parameters

Note: this distribution is rather flexible and takes on many different forms depending on the parameters, for our illustration, emphasis will be placed on generating an over-dispersed example to illustrate the calculations. Here, prob=0.35 represent the probability a given plant would be infected, based on Bernoulli trials and subsequently the beta-distribution

```{r}
set.seed(10000)
field2<-rbetabinom(n=300, prob=0.35, size=10, shape1=0.2, shape2=2)

field2.matrix<-matrix(field2,nrow=15,ncol=20)
field2.matrix

map1_long <- 
  reshape2::melt(field2.matrix, varnames = c("rows", "cols")) 
# See first rows onf melted matrix
head(map1_long)
point_size <- 11
map1_long %>% 
  ggplot(aes(factor(cols), factor(rows), 
             color = value,
             label = as.character(value)))+
  geom_point(size = point_size, shape = 15)+
  geom_text(colour = "black")+
  scale_color_gradient("Health status (counts of infected plants):",
                       low = "lightgreen", high = "#993300")+
  theme_bw()+
  theme(axis.title = element_blank(),
        legend.position = "top",
        panel.background = element_rect(terrain.colors(5)[3]),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  coord_equal()
```

Generate a sample of size 25 that represents the pilot study.

```{r}
set.seed(602)
field2.sample<-sample(x=field2, size=25)
field2.sample
```

<!-- ## Calculations for mean, variance, and variance based on binomial distribution -->

<!-- First, let's calculate disease incidence as a proportion followed by a comparison of the variances -->

<!-- ```{r} -->
<!-- field2.sampleB<-field2.sample/10 -->
<!-- field2.mean<-mean(field2.sampleB) -->
<!-- field2.var<-var(field2.sampleB) #Variance based on normal distribution -->
<!-- field2.varbin<-(field2.mean*(1-field2.mean))/10 -->
<!-- ``` -->

<!-- Index of dispersion (D) -->

<!-- ```{r} -->
<!-- var.relation<-field2.var/field2.varbin -->
<!-- ``` -->

<!-- Question: What does index of dispersion stand for? -->

<!-- ```{r} -->
<!-- N1<-(1-field1.mean)/(10*field1.mean*0.1^2) -->
<!-- N1 -->

<!-- N2<-(1-field1.mean)/(10*field1.mean*0.2^2) -->
<!-- N2 -->
<!-- ``` -->

<!-- Exercise: Draw a new sample of 25 and run the same calculations to determine new sample sizes based on CV = 0.1 and CV = 0.2\ -->
<!-- Hint: `set.seed()` -->

<!-- ## Sample Size Calculations 1 -->

<!-- Based on previous knowledge of the Power law relationship, values were calculated to represent to two parameters, A and B (see Madden et al. for more details) Parameters: A=16; B=1.4 -->

<!-- ```{r} -->
<!-- N1<-(1-field1.mean)/(10*field1.mean*0.1^2) -->
<!-- N1 -->

<!-- N2<-(1-field1.mean)/(10*field1.mean*0.2^2) -->
<!-- N2 -->
<!-- ``` -->

<!-- Define parameters: -->

<!-- ```{r} -->
<!-- A<-16 -->
<!-- B<-1.4 -->
<!-- CV1 <- .1 -->
<!-- CV2 <- .2 -->
<!-- ``` -->

<!-- Calculate the sample size. -->

<!-- ```{r} -->
<!-- (N.power1<-(A*field2.mean^(B-2)*(1-field2.mean)^B)/(10^B*CV1^2)) -->
<!-- (N.power2<-(A*field2.mean^(B-2)*(1-field2.mean)^B)/(10^B*CV2^2)) -->
<!-- ``` -->

<!-- ## Sample Size Calculations 2 -->

<!-- Based on design effect (deff) and the beta-binomial distribution calculations. We use here the var.relation to represent the empirical heterogeneity factor (deff) -->

<!-- ```{r} -->
<!-- deff = 2.54 -->
<!-- CV1 <- .1 -->
<!-- CV2 <- .2 -->

<!-- N.betabinom1 <- ((1 - field2.mean) * deff) / (10 * field2.mean * CV1 ^ 2) -->
<!-- N.betabinom2 <- ((1 - field2.mean) * deff) / (10 * field2.mean * CV2 ^ 2) -->
<!-- ``` -->






<!-- ## Based on PROPORTION OF MEAN, H, where M(population size) is NOT defined -->

<!-- ```{r func2} -->
<!-- SRS.PROPORTION.1 <- function (mean,z,H) { -->
<!--   Nest<-((1-mean)/mean)*(z/H)^2 -->
<!--   names(Nest)<-"SampleSize" -->
<!--   return(Nest) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r plotlyplot2, fig.width=6, fig.height=5} -->
<!-- gg <- -->
<!--   as.data.frame(expand.grid( -->
<!--     mean = seq(.05, .3, .05), -->
<!--     z = 1.96, -->
<!--     H =  seq(.1, .5, .05) -->
<!--   )) %>% -->
<!--   mutate(SampleSize = SRS.PROPORTION.1(mean, z, H)) %>% -->
<!--   ggplot(aes(H, SampleSize, group = mean, color = mean)) + -->
<!--   geom_point(size = .5) + -->
<!--   geom_line() + -->
<!--   scale_color_viridis_c() + -->
<!--   theme_bw() -->

<!-- plotly::ggplotly(gg)  -->
<!-- ``` -->

<!-- ## Based on PROPORTION OF MEAN, H, where M (population size) is defined -->

<!-- -   *M* = population size\ -->
<!-- -   *mean* = mean disease incidence\ -->
<!-- -   *z* = desired value for Z (typically for our needs, 1.96 will suffice) -->
<!-- -   *H* = a proportion of the mean, values from the literature range, empirically, from 0.1 to 0.5 -->

<!-- ```{r func3} -->
<!-- EstNfcf <- function (M,mean,z,H) { -->
<!--   return(((1-mean)/mean)*(z/H)^2) -->
<!-- } -->
<!-- EstFcf <- function (M,mean,z,H) { -->
<!--   Nest<-((1-mean)/mean)*(z/H)^2 -->
<!--   finite <-Nest/M -->
<!--   Nsample <-ifelse(finite>0.1,Nest/(1+(Nest/M)),Nest) -->
<!--   return(round(Nsample,digits=1)) -->
<!-- } -->
<!-- NestM <- function (M,mean,z,H) { -->
<!--   Nest <-((1-mean)/mean)*(z/H)^2 -->
<!--   finite <-Nest/M -->
<!--   return(round(finite, digits=2)) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r data_prep3, fig.width=9, fig.height=7.5} -->
<!-- gg <-  -->
<!--   as.data.frame(expand.grid( -->
<!--     mean = seq(.05, .3, .05), -->
<!--     z = 1.96,  -->
<!--     H =  seq(.1,.5,.05), -->
<!--     M =1000 -->
<!--   ) ) %>%  -->
<!--   mutate(  -->
<!--     est_nfcf = EstNfcf(M,mean,z,H), -->
<!--     est_fcf = EstFcf(M,mean,z,H), -->
<!--     nest_m = NestM(M,mean,z,H) -->
<!--   ) %>%  -->
<!--   mutate("NEST/M" = nest_m , -->
<!--          "Estimated-FCF" = est_fcf, -->
<!--          "Estimated-NFCF" = est_nfcf) %>%  -->
<!--   pivot_longer(cols = c("Estimated-NFCF", "Estimated-FCF", "NEST/M")) %>%  -->
<!--   ggplot(aes(H, value, group = mean, color = mean))+ -->
<!--   geom_point(size = .5)+ -->
<!--   geom_line()+ -->
<!--   scale_color_viridis_c()+ -->
<!--   facet_wrap(~name, ncol = 1, scales = "free_y")+ -->
<!--   theme_bw()+ -->
<!--   theme(legend.position = "top") -->
<!-- ``` -->

<!-- ```{r plotlyplot3, fig.width=9, fig.height=7.5} -->
<!-- plotly::ggplotly(gg) %>%  -->
<!--   plotly::layout(legend = list(orientation = "h", x = 0.4, y = -0.2)) -->
<!-- ``` -->

<!-- # Based on Fixed positive number, h, where M IS NOT defined (population size) -->

<!-- *h* represents the half length of a confidence interval based on a fixed positive number -->

<!-- ```{r func4 } -->
<!-- SRS.PROPORTION.1 <- function (mean,z,h) { -->
<!--   Nest<-(mean*(1-mean))*(z/h)^2 -->
<!--   names(Nest)<-"SampleSize" -->
<!--   return(Nest) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r plotlyplot4, fig.width=6, fig.height=5} -->
<!-- gg <-  -->
<!-- as.data.frame(expand.grid( -->
<!--   mean = seq(.05, .3, .05), -->
<!--   z = 1.96,  -->
<!--   h =  seq(.1,.5,.05) -->
<!-- ) ) %>%  -->
<!--   mutate(  -->
<!--     SampleSize = SRS.PROPORTION.1(mean,z,h) -->
<!--   ) %>%  -->
<!--   ggplot(aes(h, SampleSize, group = mean, color = mean))+ -->
<!--   geom_point(size = .5)+ -->
<!--   geom_line()+ -->
<!--   scale_color_viridis_c()+ -->
<!--   theme_bw() -->

<!-- plotly::ggplotly(gg)  -->
<!-- ``` -->



<!-- # CLUSTER SAMPLING FOR DISEASE INCIDENCE -->

<!-- The following section has functions to calculate sample sizes based on Madden et al. (2007), table 10. Note, for the following, the finite population is not applied; this could be handled using direct calculations. Based on known number of sampling units, etc., or code could be modified accordingly. -->

<!-- ## Based on CV -->

<!-- -   *mean* = mean disease incidence\ -->
<!-- -   *n* = number of individuals per sampling unit -->
<!-- -   *CV* = coefficient of variation -->
<!-- -   *N* = number of sampling units (e.g., quadrant) estimated based on outlined criteria -->

<!-- ```{r func5} -->
<!-- EstNfcf <- function (mean, n, CV, M) { -->
<!--   return(round((1 - mean) / (n * mean * CV ^ 2), 2)) -->
<!-- } -->
<!-- EstFcf <- function (mean, n, CV, M) { -->
<!--   Nest <- (1 - mean) / (n * mean * CV ^ 2) -->
<!--   finite <- Nest / M -->
<!--   Nsample <- ifelse(finite > 0.1, Nest / (1 + (Nest / M)), Nest) -->
<!--   return(round(Nsample, digits = 1)) -->
<!-- } -->
<!-- NestM <- function (mean, n, CV, M) { -->
<!--   Nest <- (1 - mean) / (n * mean * CV ^ 2) -->
<!--   finite <- Nest / M -->
<!--   return(round(finite, digits = 2)) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r data_prep5, fig.width=9, fig.height=7.5} -->
<!-- gg <- -->
<!-- as.data.frame(expand.grid( -->
<!--   mean =seq(.1, .9, .1), -->
<!--   n = 10, -->
<!--   CV =seq(.05, .3, .01),  -->
<!--   M =  c(100, 500, 1000) -->
<!-- )) %>% -->
<!--   mutate( -->
<!--     est_nfcf = EstNfcf(mean, n, CV, M), -->
<!--     est_fcf = EstFcf(mean, n, CV, M), -->
<!--     nest_m = NestM(mean, n, CV, M) -->
<!--   ) %>% -->
<!--   mutate("NEST/M(X100)" = nest_m *100, -->
<!--          "Estimated-FCF" = est_fcf, -->
<!--          "Estimated-NFCF" = est_nfcf) %>%  -->
<!--   pivot_longer(cols = c("Estimated-NFCF", "Estimated-FCF", "NEST/M(X100)")) %>%  -->
<!--   ggplot(aes(CV, value, group = mean, color = mean))+ -->
<!--   geom_point(size = .5)+ -->
<!--   geom_line()+ -->
<!--   scale_color_viridis_c()+ -->
<!--   facet_grid(M~name)+ -->
<!--   theme_bw()+ -->
<!--   theme(legend.position = "top") -->
<!-- ``` -->

<!-- ```{r plotlyplot5, fig.width=9, fig.height=7.5} -->
<!-- plotly::ggplotly(gg) %>%  -->
<!--   plotly::layout(legend = list(orientation = "h", x = 0.4, y = -0.2)) -->
<!-- ``` -->

<!-- ## Based on proportion of mean, H -->

<!-- -   *mean* = mean disease incidence\ -->
<!-- -   *n* = number of individuals per sampling unit -->
<!-- -   *z* = desired value for Z (typically for our needs, 1.96 will suffice) -->
<!-- -   *H* = a proportion of the mean, values from the literature range, empirically, from 0.1 to 0.5 -->

<!-- ```{r func6} -->
<!-- CLUSTER.RANDOM.2 <- function (mean, n, z, H) { -->
<!--   Nest<-((1-mean)/(n*mean))*(z/H)^2 -->
<!--   names(Nest)<-"SampleSize" -->
<!--   return(Nest) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r plotlyplot6, fig.width=6, fig.height=5} -->
<!-- gg <- -->
<!--   as.data.frame(expand.grid( -->
<!--     mean = seq(.05, .3, .05), -->
<!--     n = 10, -->
<!--     z = 1.96, -->
<!--     H =  seq(.1, .5, .05) -->
<!--   )) %>% -->
<!--   mutate(SampleSize = CLUSTER.RANDOM.2(mean, n, z, H)) %>% -->
<!--   ggplot(aes(H, SampleSize, group = mean, color = mean)) + -->
<!--   geom_point(size = .5) + -->
<!--   geom_line() + -->
<!--   scale_color_viridis_c() + -->
<!--   theme_bw() -->

<!-- plotly::ggplotly(gg)  -->
<!-- ``` -->

<!-- ## Based on fixed positive number, H -->

<!-- -   *mean* = mean disease incidence\ -->
<!-- -   *n* = number of individuals per sampling unit -->
<!-- -   *z* = desired value for Z (typically for our needs, 1.96 will suffice) -->
<!-- -   *h* = half length of the confidence interval -->

<!-- ```{r func7} -->
<!-- CLUSTER.RANDOM.3 <- function(mean, n, z, h) { -->
<!--   Nest <- ((mean * (1 - mean)) / n) * (z / h) ^ 2 -->
<!--   names(Nest) <- "SampleSize" -->
<!--   return(Nest) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r plotlyplot7, fig.width=6, fig.height=5} -->
<!-- gg <-  -->
<!-- as.data.frame(expand.grid( -->
<!--   mean = seq(.05, .3, .05), -->
<!--   n = 10, -->
<!--   z = 1.96,  -->
<!--   h =  seq(.01,.1,.01) -->
<!-- ) ) %>%  -->
<!--   mutate(  -->
<!--     SampleSize = CLUSTER.RANDOM.3(mean,n, z,h) -->
<!--   ) %>%  -->
<!--   ggplot(aes(h, SampleSize, group = mean, color = mean))+ -->
<!--   geom_point(size = .5)+ -->
<!--   geom_line()+ -->
<!--   scale_color_viridis_c()+ -->
<!--   theme_bw() -->

<!-- plotly::ggplotly(gg)  -->
<!-- ``` -->
