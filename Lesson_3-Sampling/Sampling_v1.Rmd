---
title: "Sampling"
author: "Mladen Cucak; Felipe Dalla Lana; Mauricio Serrano; Paul Esker, Miranda DePriest"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float:
      toc_depth: 5
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


# **Introduction** 

Sampling, or determining which individuals to collect data from, is an important aspect of designing and conducting an experiment. Sampling is often necessary because collecting data from a whole population can be impossible. The method of sampling you chose can determine the quality of your data, the statistical analyses you can use, and impact the reliability of any insights from your experiment. The sampling method employed can often depend on the size and structure of the population, the resources you have, and the type of questions you are asking. 

In this lesson, we will examine many of the most common sampling methods. In each section, you will learn about how to employ the sampling method and when to use it. We will also discuss how to assess the reliability of the results we get when sampling.


### **Setting up**

First, we need to set up our R session. In the next section of code, we will load the packages we need for this lesson. If you encounter difficulty, please see Lesson 1 - Intro to R. 

```{r libs, message=FALSE, warning=FALSE,class.source = 'fold-show'}

# First, we will set a CRAN 'mirror'. This directs R to install the following packages from a certain CRAN repository ('repo'). Normally, R automatically sets the CRAN repo, but sometimes using .Rmd files (instead of .R files) can cause problems.
local({r <- getOption("repos")
       r["CRAN"] <- "https://cran.r-project.org" 
       options(repos=r)
})


# Instead of doing this:
# library("dplyr")
# library("tidyr")
# library("ggplot2")
# ...
# ...

# We will create a list of desired packages and pass it to the `library()` command. 

# Make the list of desired packages
list.of.packages <-
  c("tidyverse", # Remember, tidyverse contains several packages, including ggplot2
    "conflicted", 
    "here", 
    "plotly", 
    "emdbook", 
    "ROCR"
  )

# Make a list of any packages that aren't installed
new.packages <-
  list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]

# If the list of un-installed packages has any packages listed, install them
if (length(new.packages))
  install.packages(new.packages)

# Load all packages into this R session
packages_load <-
  lapply(list.of.packages, require, character.only = TRUE)

# Print a warning if there is a problem with installing/loading some of packages
if (any(as.numeric(packages_load) == 0)) {
  warning(paste("Package/s: ", paste(list.of.packages[packages_load != TRUE], sep = ", "), "not loaded!"))
} else {
  print("All packages were successfully loaded.")
}

# Remove the package-related objects we created
rm(list.of.packages, new.packages, packages_load)

# Resolve conflicts (identical commands with different functions) between packages. 
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("layout", "plotly")

# If installation is not working, try changing repos. CRAN repos can be found here: https://cran.r-project.org/mirrors.html 

```


# **Evaluating sampling reliability**

Sample data is used to estimate the characteristics of the entire population. In plant pathology, you could evaluate the disease severity of a sample of plants to predict how the whole population of plants will respond. Because the whole population could not be surveyed, there is **inherent uncertainty**. How can we improve the *reliability* of the result? How can we predict *how much sampling* (the reliable sample size) is needed to create a reliable result?  

> **Reliability**: A result can be considered reliable when it can be replicated. In other words, this is when random re-sampling produces similar results.


### **The coefficient of variation (CV)**

The reliability of a result can depend on the *variation* between observations. In our case, this could be the differences in disease severity among sampled plants. The spread of these values can determine how much sampling is necessary to form a reliable result. Variation in disease severity among plants can come from intentional experimental factors, like treatment, or from random events, such as the differences between individuals. 

To measure variability, we can calculate the **coefficient of variation** (CV). The CV can be calculated by:

CV = σ / μ

where σ is the standard deviation in the data and μ is the mean. Essentially, we can interpret the CV as the ratio between the standard deviation and the mean. 

**Note:** The standard deviation is equivalent to the square root of the variance. 

**Example 1.1: Calculating the CV of a simple data set**

In this example, we survey 50 plants for the presence or absence of disease. 

```{r cv_calc,class.source = 'fold-show'}

# First, we create the example data:

# Total number of plants
n_plants <- 50 

# Number of diseased plants
infected <- 20 


# Now, we'll calculate the CV of the data

# The incidence (mean) of disease
inc <- infected/n_plants
inc

# We calculate the variance in disease incidence
var <- (inc* (1 - inc))/n_plants
var
# Note - R has a function for calculating variance, but it's important to see how it is calculated. 

# We use the standard deviation (square root of the variance) and the mean to calculate the CV
cv <- sqrt(var)/inc
cv

```

In this example data set, we got a CV of 0.173 (17.3%). In plant pathology research, values between 0.1 and 0.2 (10% to 20%) are typically considered satisfactory.

### **The index of dispersion (ID)**

The index of dispersion (ID) is related to the CV. It is a ratio determined by the variance and mean. The distribution pattern of the data is important when interpreting the ID. In data that has a binomial distribution, an ID of less than 1 is considered ideal. 

The index of dispersion can be calculated by:

ID = var^2^/mean

**Example 1.2: We can use the previous example to show the calculation of an ID:**

```{r id_calc,class.source = 'fold-show'}

# Use the variance (var) and mean (inc) to calculate the ID
id <- (var^2)/inc
id

```

### **Confidence interval half-width (CI half-width)**

We can also use the range of the confidence interval as an indicator of reliability. 

Confidence intervals (CIs) represent the probability that a random observation will fall between two points. Just like the CV, CIs depend on data standard deviation and mean. Typically, we will use a CI that encompasses at least 95% of the data. This interval will contain values that differ by less than 1.96 standard deviations from the mean. The half width of this interval will be equivalent to the standard deviation multiplied by 1.96. 

CI half width (95%) = 1.96 * var^1/2^

**Example 1.3: Calculating the half width of the 95% CI**

We will use the previous example data to calculate the CI half-width. For this example, we will calculate the half-width of the CI that encompasses 95% of the data. Results that fall within half of this width are considered reliable. 

```{r half_width_calc,class.source = 'fold-show'}

# Half-width of the required confidence interval
halfCI <- 1.96*sqrt(var) 
halfCI

```

This means that for the incidence rate of 0.4 (40%) to be considered a reliable result, results from random re-sampling must fall within 0.4 $\pm$ 0.135 (13.5%). 

### **Finite population correction factor (FPC)**

We must also consider the impact of sampling a relatively large portion of the population without replacing specimens back into the sampling pool. As specimens are sampled and 'removed' from the pool of available specimens to sample, the remaining population changes in composition. Typically, finite population correction is necessary when sampling 10% or more of the total population. 

The finite population correction *factor* (FPC) is calculated by:

FPC = ((N-n)/(N-1))^(1/2)^

where N represents the total population and n represents the sampled population. The FPC is bounded between 0 and 1, and decreases as sampling efforts impact the remaining population. Typically, if the FPC is below 0.95 (95%), the size of the sampled individuals needs to be addressed. 


**Example 1.4: Calculating the FPC by sampling effort in a population of 10,000**

In the next example, we are going to see how the number of individuals sampled impacts the FPC. The example population includes 10,000 individuals.  

```{r fpc_calc}

# Create the example data
df <- data.frame(n = seq(0, 10000, by = 500),
                 N = 10000) %>%
  mutate(prop = (n/N)*100, # This represents the proportion of the population sampled
         fpc = ((N-n)/(N-1))^(1/2))
df


ggplot(df, aes(prop, fpc)) +
  geom_point() +
  xlab("Proportion of surveyed individuals (%)") +
  ylab("FPC")


```


In the graph above, we can see that at smaller sampling ranges, the FPC remains close to 1. This means that the impact of sampling without replacement on the remaining population is negligible. However, the FPC drops below 0.95 when more than 10% of the population is being sampled without replacement. There are a number of methods to use if this becomes this case, such as sampling *with* replacement or by simply reducing your sampling effort. The choice of method will depend on the situation.

**Note:** We will initially distribute the above calculations with data with a binomial distribution. This describes data with two outcomes, where the likelihood of each outcome is recorded. Keep in mind that the calculation and interpretation of these metrics may change based on the distribution of the data. 

# **Sampling methods** 

Now we will discuss common sampling methods, particularly randomized sampling and clustered/blocked sampling. We will also provide examples for when the use of a certain method is appropriate. The sampling method you choose to use is important in reducing elements that can bias your findings. 

Some things that can bias the results of a study are:
> Sampling bias: This is when certain individuals are selected for data collection according to the researcher's conscious or unconscious bias. 

Sampling bias example 1 - A researcher chooses not to select individuals from a part of a field that is hard to get to. Random sampling can ensure that individuals from this part of the field are as likely to be collected as any other individual in the experiment.

Sampling bias example 2: Although the researcher does not know it, a certain proportion of the field has been subjected to different environmental conditions. Random sampling improves the chances of collecting a representative number of individuals from that area of the field compared to the rest of the field. This way, the results of the experiment are less likely to attribute any variation in plants to the treatment when in reality they were due to the different field conditions. 


### **Simple random sampling (SRS)**

Simple random sampling (SRS) is the most basic way to implement randomness in a sampling plan. Individuals at selected at random for collecting data, removing things that can introduce bias. 

For the following example, please imagine that we are sampling to determine what percent of the population has disease, with the true disease incidence being 41%. 

**Example 3: SRS for accurately sampling disease severity ** 

In this example, we will sample randomly without replacement. This is because, as individuals are surveyed and removed from the population, the population doesn't not change significantly. We can use base R to obtain a list of randomly selected plants to sample. Then, we will find our resulting disease incidence and use the CV to see if our results are reliable. 

```{r srs, class.source = 'fold-show'}

# Let's create our example data, where 0 is no disease and 1 is disease
pop <- data.frame(plant_id = 1:1000) %>%
  mutate(status = sample(c(0, 1), n(),
                        replace = TRUE, prob = c((1-0.41), 0.41)))
head(pop)

# We will use `set.seed()` to generate a pseudorandom number so that the class gets the same results. This is a great way to ensure that your work is reproducible while still getting randomly selected numbers later. The number you pick isn't important, but recording it is! 
set.seed(1866844)

# We will sample 25 plants
n = 25

# Use base R function `sample()` to pick specimens without replacement
sampled.plants <- sample(pop$plant_id, n, replace = FALSE)

# Create a data frame of only the sampled plants
sampled.df <- pop[sampled.plants,]

# Calculate the incidence of disease in the sampled plants
inc <- sum(sampled.df$status)/n
inc

# Calculate the variance
var <- (inc * (1-inc))/n
var

# Calculate the CV 
cv <- sqrt(var)/inc
cv 

```


We got a disease incidence rate of 52%, which is a bit off compared to the true value of 41%. The CV was within 20%, so it may be considered satisfactory. But what if this research was in a field where 20% wasn't considered satisfactory, and we had to plan how many samples to take to get a CV of approximately 10% or less? 

We can use the following formula to estimate the number of samples we'll need for a CV of 10% or less:

n = (1-inc)/(inc*0.1^2^)

Let's see approximately how many samples we would need. 


```{r srs_nsamples, class.source = 'fold-show'}

# Calculate the number of samples needed
n0 <- (1-inc)/(inc*0.1^2)
n0

```

We would need approx. 92 plants to get a CV of 10% or less. 


### **Cluster sampling**

Cluster sampling is a method of sampling where sub-sections ("clusters") of the whole population are designated for sampling. This method can be appropriate in instances where the population is divided, such as when fields are split into quadrants, or when an experiment takes place over multiple fields. Clusters are usually determined randomly, but in some scenarios, researchers must choose clusters based on elements like accessibility. Randomness can also be used when sampling *within* a cluster by using a simple random sampling technique to select individuals from a cluster. 

Cluster sampling can be great when visiting each location in an experiment is costly, but the validity of the results are dependent on those clusters accurately representing the population. For plant pathology research, this often means that every treatment must be represented in every cluster. 

**Example 4.1: Cluster sampling for disease severity**

In this example, we will imagine that we have an experiment consisting of 300 quadrants (20 x 15), with 10 plants each. The true value for the incidence of disease is 30%. We are interested in determining the incidence of disease, but we can't sample every quadrant. 

We will visualize the disease incidence in each quadrant by building a matrix in R. Then, we will randomly sample 10 quadrants and evaluate our results. 

```{r cluster_sampling, class.source = 'fold-show'}

# Use set.seed() for reproducibility
set.seed(8486)

# Create normally distributed data showing how many plants are diseased in each quadrant
field <- rbinom(n = 300, size = 10, prob = 0.30)
head(field)

# Provide the column and row of each quadrant in a data frame with the disease incidence
field1 <- data.frame(rows = rep(1:20, times = 15),
                    cols = rep(1:15, each = 20),
                    n.dis = field)
head(field1)

# Now we can plot our quadrants and show the number of plants with disease in each
ggplot(field1, aes(x = cols, y = rows, color = n.dis, 
                  label = as.character(n.dis))) +
  geom_point(size = 10, shape = 15) +
  geom_text(colour = "black") +
  scale_color_gradient("Number of infected plants",
                       low = "lightgreen", high = "orangered") +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "top",
        panel.background = element_rect(terrain.colors(5)[3]),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  coord_equal()

# Set seed 
set.seed(46846811)

# We will sample 10 quadrants with 10 plants each
q = 10
n = 100

# Randomly pick 10 quadrants to sample
sampled.plants <- sample(300, q, replace = FALSE)
sampled.plants

# Create a data frame with only sampled plants
sampled.df <- field1[sampled.plants,]
sampled.df

# Get the disease mean
inc <- sum(sampled.df$n.dis)/n
inc

# Calculate the variance
var <- (inc * (1-inc))/n
var

# Calculate the CV 
cv <- sqrt(var)/inc
cv

# Calculate the number of plants needed for CV of 10%
n0 <- (1-inc)/(inc*0.1^2)
n0

# Calculate the number of quadrants we would need (rounded up)
q0 <- ceiling(n0/10)
q0

# Calculate the ID
id <- (var^2)/inc
id

```


To demonstrate the differences in reliability metrics based on data distribution, we will consider a similar example. In this example,  the data is 'overdispersed'. Data is considered over-dispersed when the variance of the response (the variable we are collecting data on) is greater than what is assumed in the model. 

To create this example data, we will introduce a new element: \$\theta. This is the 'overdispersion parameter'. It will impact two of arguments we'll use to create the example data, shape1 and shape2. 

**Example 4.2: An example in overdispersed data**

To demonstrate the differences in reliability metrics based on data distribution, we will consider an almost-identical example. In this example,  the data is 'overdispersed'. Data is considered over-dispersed when the variance of the response (the variable we are collecting data on) is greater than what is assumed in the model. 

To create this example data, we will introduce a new element: \$\theta. This is the 'overdispersion parameter'. It will impact two of arguments we'll use to create the example data, shape1 and shape2. 

```{r srs_overdisp, class.source = 'fold-show'}

# Use set.seed() for reproducibility
set.seed(8486)

# Create normally distributed data showing how many plants are diseased in each quadrant
field <- rbetabinom(300, prob = 0.30, size = 10,
                    shape1 = 0.2, shape2 = 2)
head(field)

# Provide the column and row of each quadrant in a data frame with the disease incidence
field1 <- data.frame(rows = rep(1:20, times = 15),
                    cols = rep(1:15, each = 20),
                    n.dis = field)
head(field1)

# Now we can plot our quadrants and show the number of plants with disease in each
ggplot(field1, aes(x = cols, y = rows, color = n.dis, 
                  label = as.character(n.dis))) +
  geom_point(size = 10, shape = 15) +
  geom_text(colour = "black") +
  scale_color_gradient("Number of infected plants",
                       low = "lightgreen", high = "orangered") +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "top",
        panel.background = element_rect(terrain.colors(5)[3]),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  coord_equal()

# Set seed 
set.seed(46846811)

# We will sample 10 quadrants with 10 plants each
q = 10
n = 100

# Randomly pick 10 quadrants to sample
sampled.plants <- sample(300, q, replace = FALSE)
sampled.plants

# Create a data frame with only sampled plants
sampled.df <- field1[sampled.plants,]
sampled.df

# Get the disease mean
inc <- sum(sampled.df$n.dis)/n
inc

# Calculate the variance
var <- (inc * (1-inc))/n
var

# Calculate the CV 
cv <- sqrt(var)/inc
cv

# Calculate the number of plants needed for CV of 10%
n0 <- (1-inc)/(inc*0.1^2)
n0

# Calculate the number of quadrants we would need (rounded up)
q0 <- ceiling(n0/10)

# Calculate the ID
id <- (var^2)/inc
id

```

We got very different values compared to Example 4.1. The distribution of this data meant that, in order to get a CV of 0.1, we would have had to sample more quadrants than we have. In reality, this kind of distribution is rare, but it's good to think about what these values mean in different contexts. 

### **Stratified sampling: Combining cluster and simple random sampling**

So far, we have implemented randomness in determining which quadrants to sample in a clustered sampling regime. We can add simple random sampling *within* each quadrant for a multi-phase sampling approach. This might be appropriate when quadrants are large, or sampling resources are restrictive. 

**Example 4.3: Combining clustered and random sampling**

For this example, we will modify the situation in Example 4.1 - the example with cluster sampling and data *without* overdispersion. The difference will be in the number of plants per quadrant (n). The number of plants in each cluster will be 100. Instead of sampling 100 plants (10 quadrants, 10 plants each), you would be sampling 1000 plants (10 quadrants, 100 plants each). 

Let's imagine that we don't have the resources to sample that many plants. How do we choose plants to sample in each quadrant in a way that manages bias? The answer is by using a simple random sampling technique within each quadrant. 

````{r stratified, class.source = 'fold-show'}

# First, we'll create data with quadrant ID and disease status (diseased or un-diseased) for # each plant. We will need quad and plant to be characters later on, so we'll change them #now.
field <- data.frame(quad = as.character(rep(1:300, each = 100)),
                    plant = as.character(rep(1:100, times = 300))) %>%
                    mutate(dis = sample(c(0,1), n(), 
                                  replace = TRUE, 
                                  prob = c((1-0.3), 0.3)))
            
# Set seed 
set.seed(46846811)

# Randomly pick 10 quadrants to sample
quads <- sample(300, 10, replace = FALSE) 
quads

# Randomly pick 10 plants out of every quadrant
set.seed(45)
plants <- sample(100, 10, replace = FALSE)

# We'll subset the data to only include plants we will sample
field1 <- field[which(field$quad == quads[1] | field$quad == quads[2] |
                      field$quad == quads[3] | field$quad == quads[4] |
                      field$quad == quads[5] | field$quad == quads[6] |
                      field$quad == quads[7] | field$quad == quads[8] |
                      field$quad == quads[9] | field$quad == quads[10]),]
field2 <- field1[which(field1$plant == plants[1] | field1$plant == plants[2] |
                      field1$plant == plants[3] | field1$plant == plants[4] |
                      field1$plant == plants[5] | field1$plant == plants[6] |
                      field1$plant == plants[7] | field1$plant == plants[8] |
                      field1$plant == plants[9] | field1$plant == plants[10]),]
                      
# Calculate the mean disease occurrence (inc)
inc <- sum(field2$dis)/100
inc

# Variance
var <- (inc * (1-inc))/100
var

# CV 
cv <- sqrt(var)/inc

```

This combined sampling regime resulted in a respectable CV of 0.139, and we only had to sample 100 plants. 

We can now introduce the concept of the **design effect** ('deff'). This is an estimate of the precision lost or gained through use of stratified sampling. Essentially, it is the ratio of of the variance from the stratified sampling plan (in the previous example, clustered sampling with random sampling within clusters), to the variance you would get from simple random sampling alone. We'll touch on this concept more when talking about experimental design, but know that the design effect can be useful in determining how to allocate your sampling units between stratified sampling layers. 

We will show how knowledge of the design effect can impact the samples you will need to collect. We will assume we already know the value for deff from pilot studies.

```{r deff, class.source = 'fold-show'}

# Define deff
deff = 2.54

# Calculate how deff will impact the number of samples needed for a CV of 0.1
N <- ((1-inc)*deff)/(10*inc*0.1^2)
N

```

# **Determining sample size based on incomplete knowledge**

So far, we have used the variance, mean, and population size to calculate necessary sample size (n) to get a certain value of CV. In the real world, you may not always have these variables. When the unknown variable is population size, you need to consider whether sampling with replacement is necessary. We'll talk about how to proceed when different variables are missing. 

### **Determining sample size without the mean**

In many cases, we may find ourselves without knowledge of the mean, especially when we're deciding how intensively to sample. In this case, we can estimate the mean with minimal effort and then create a plan for our full sampling effort. 

**Example 5.1: Calculating n without the mean**

```{r n_wo_mean, class.source = 'fold-show'}

# Let's create some example data
pop <- data.frame(plant_id = 1:1000) %>%
  mutate(status = sample(c(0, 1), n(),
                        replace = TRUE, prob = c((1-0.65), 0.65)))
head(pop)

# Set seed
set.seed(746814)

# Randomly select 10 samples
sampled.plants <- sample(pop$plant_id, 10, replace = FALSE)

# Subset the selected plants
sampled.df <- pop[sampled.plants,]

# Calculate the mean of these 10 samples
approx.inc <- mean(sampled.df$status)
approx.inc

# Use the estimated mean to estimate the variance
var <- (approx.inc*(1-approx.inc))/(10)
var

# Now we can create a 95% CI to see the range that the population mean falls in
lower.bound <- approx.inc - 0.98*sqrt(var) 
upper.bound <- approx.inc + 0.98*sqrt(var)

# And then proceed to calculate n normally
n <- (1-inc)/(inc*0.1^2)
n

```

By examining the scores of only a few plants, we improved our estimation of the mean and determined how many plants we would need to sample. 


**Example 5.2: Calculating n without the population size (N)**

For estimations where the population size is not known, we can use the mean (inc), variance, the margin of error (MOE), and the Z score. 

```{r n_wo_pop, class.source='fold-show'}

# Define inc and var
inc <- 0.3
var <- .002

# Estimate the margin of error within 2 standard deviations of the mean
moe <- 1.96*sqrt(var)
moe

# Estimate n 
n <- (1.96*inc*(1-inc))/moe^2
n

```

