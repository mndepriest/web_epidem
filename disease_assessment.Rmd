---
title: "Disease assessment: Accuracy and precision"
author: "Paul Esker; Felipe Dalla Lana; Mladen Cucak, Miranda DePriest"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      toc_depth: 3
      collapsed: false
      smooth_scroll: false
---

# **Introduction**

<!-- This is the lab section of the class of disease assessment. In this document you will learn some analyses on how to quantify precision and reliability when measuring disease intensity. We will use R for this activity and data are provided from nine raters unfamiliar with the disease that was used. Each rater measured foliar disease severity of 20 trifoliate leaves, with a shape similar to peanuts or soybean.  Leaves used in this exercise were generated by the software *Severity.Pro*, developed by Nutter et al. Raters measured the disease severity on their computers, with each leaf being measure twice in two different randomly assigned sets. -->


The ability to rate disease severity is a crucial tool in plant epidemiological research. Determining disease severity can provide valuable information on which treatments are effective. However, disease ratings can be subjective, and vary from person to person. Information about the *quality* of disease ratings can impact how an analysis is performed. In this lesson, we will learn about several measures that reflect how good a set of ratings are. 

For this exercise, we use an example data set. **Twenty trifoliate leaves** were rated by **nine participants**. Each trifoliate **appeared twice** for each participant in a random order. Because there are 18 disease severity ratings for *each* trifoliate, we can evaluate the quality of the ratings. Before we get to the example, it's important to know some terms.


# **Important terms: Precision and Accuracy**


> **Precision**, or reliability, is a measure of the *similarity* of multiple ratings on a test subject. 

> **Accuracy** is a measure of how *close* the rating value is to the 'true' value.

The following figure serves as an illustration of precision and accuracy in disease severity ratings. The center of each dartboard, the 'bullseye', represents the 'true' value. Whether the true value can be determined or not depends on the rating system. In this case, we will assume that we know the true value of the rating. **Precision** is the distance between individual darts. **Accuracy** is the distance between darts and the true value. 

**Top left** In the top left example, there is both poor precision and poor accuracy. The ratings were very different (poor precision) and not near the true value (poor accuracy). The value you would get from these points would not reflect the true value accurately, and would show a lot of variation between points.

**Top right** In the top right example, the ratings were similar, so the precision was good. However, they are not near the true value, so they are inaccurate. You would get less variability between ratings, but the value you would get would not be very close to the truth.

**Lower left** In the bottom left example, the values landed in different areas, giving poor precision. They were all roughly equidistant from the center, showing good accuracy. The result you would get from these points would be close to the true value, but the variability between ratings would be high.

**Lower right** in the bottom right example, the ratings are both highly precise and highly accurate. The result is an accurate and reliable measure of the true value.


```{r fig.height=8, fig.width=8, message=FALSE, warning=FALSE, dpi=300}

library(tidyverse)
library(patchwork)
library(knitr)
library(kableExtra)
library(ggsci)
library(ggpubr)

data_acc = data.frame(
          var = letters[1:6],
          value = rep(10,6))

not_prec_not_acc = 
ggplot(data_acc, aes(x = var, y = value ,fill = var )) +
  geom_bar(width = 1, stat="identity", alpha = 0.7)+
  geom_point(size = 5, shape = 21, fill = "black", color = "white", alpha = 0.9,
             aes(x = c(6.22, 6.43, 6.07, 4.82, 5.78, 6.92),
                 y = c(7.56, 5.71, 3.14, 5.92, 4.81, 2.26)))+
  coord_polar(theta = "y")+
  labs(x=NULL, y= NULL)+
  scale_fill_brewer(palette = "RdYlBu", direction =1)+
  theme(    panel.background = element_blank(),
            legend.position = "none",
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            plot.margin = margin(-70, -70, -60, -70, "pt"))
prec_acc = 
ggplot(data_acc, aes(x = var, y = value ,fill = var )) +
  geom_bar(width = 1, stat="identity", alpha = 0.7)+
    geom_point(size = 5, shape = 21, fill = "black", color = "white", alpha = 0.9,
      aes(x = c(0.56, 1.17, 0.89, 0.96, 1.04, 0.72),
          y = c(3.56, 4.71, 3.14, 3.92, 4.81, 5.26)))+
    coord_polar(theta = "y")+
    labs(x=NULL, y= NULL)+
  scale_fill_brewer(palette = "RdYlBu", direction =1)+
  theme(    panel.background = element_blank(),
    legend.position = "none",
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.margin = margin(-70, -70, -60, -70, "pt"))

prec_not_acc = 
  ggplot(data_acc, aes(x = var, y = value ,fill = var )) +
  geom_bar(width = 1, stat="identity", alpha = 0.7)+
  geom_point(size = 5, shape = 21, fill = "black", color = "white", alpha = 0.9,
             aes(x = c(4.56, 4.77, 4.89, 4.96, 5.04, 5.22),
                 y = c(4.36, 4.31, 4.14, 4.52, 4.31, 4.41)))+
  coord_polar(theta = "y")+
  labs(x=NULL, y= NULL)+
  scale_fill_brewer(palette = "RdYlBu", direction =1)+
      theme(panel.background = element_blank(),
            legend.position = "none",
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            plot.margin = margin(-70, -70, -70, -70, "pt"))


not_prec_acc = 
ggplot(data_acc, aes(x = var, y = value ,fill = var )) +
  geom_bar(width = 1, stat="identity", alpha = 0.7)+
  geom_point(size = 5, shape = 21, fill = "black", color = "white", alpha = 0.9,
             aes(x = c(2.5, 2.7, 2.3, 2.5, 2.7, 2.5),
                 y = c(0.2, 1.7, 3.4, 5.1, 6.8, 8.5)))+
  coord_polar(theta = "y")+
  labs(x=NULL, y= NULL)+
  scale_fill_brewer(palette = "RdYlBu", direction =1)+
  theme(panel.background = element_blank(),
        legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        plot.margin = margin(-70, -70, -70, -70, "pt"))


  not_prec_not_acc  + annotate("text", x = 7.0, y = 5, label = "Not Precise and Not Accurate", fontface =2, size = 5)+
  prec_not_acc      + annotate("text", x = 7.0, y = 5, label = "Precise but Not Accurate", fontface =2, size = 5)+
  not_prec_acc      + annotate("text", x = 7.0, y = 5, label = "Not Precise but Accurate", fontface =2, size = 5)+
  prec_acc          + annotate("text", x = 7.0, y = 5, label = "Precise and Accurate", fontface =2, size = 5)+
  plot_annotation(title    = 'Precision and Accuracy',
                  subtitle = 'Terms are similiar but have different meanings')&
    theme(plot.title = element_text(face="bold"))
```

# **Quantifying Precision and Accuracy**

<!-- There are multiple methods to quantify precision and accuracy. In the next sections of the document, you will learn some of the methods most frequently used by plant pathologist to quantify these different metrics considering measurements of plant disease intensity.   -->
**Statistical analyses of precision and accuracy**

Now we will discuss methods to quantify precision and accuracy, and when to use them. 

***Concordance correlation coefficient***

The concordance correlation coefficient (CCC) is a method developed by Lawrence I-Kuei Lin in 1989 (see references at the bottom of the page). The CCC is a method of determining *inter-rater* variability, or the differences between ratings done by different people. It, and other similar methods, are frequently used in plant pathology studies to measure the similarity or agreement between two groups of variables. 

In the CCC, the value ($\hat{\rho_c}$ or *CCC*) is calculated. U represents the set of ratings in question, while W represents a 'test' set of ratings to compare U to, or the true value.  

You can estimate $\hat{\rho_c}$ by the following equation:

$$ \hat{\rho_c} = \frac{2s_{UW}}{(\overline{U}-\overline{W})^2+s^{2}_{U}+s^{2}_{W}} $$
<br>

$\overline{U}$ and $s^{2}_{U}$ are the *average* and *standard deviation* of the ratings (U). Similarly, $\overline{W}$ and $s^{2}_{W}$ are the average and standard deviation of W. $s_{UW}$ is the *covariance*.


This equation can be simplified as the product of two terms:

<br>

$\hat{\rho_c} = r*C_b$. 

<br>

The first term in this simplied formula is **r**, the Pearson correlation coefficient. This value quantifies the variability between the measurements of U. It ranges from -1 to 1, where a value of 1 indicates a perfect positive association, a value of -1 is a perfect negative association, and 0 indicates no relationship. 

It's important to note that the slope of the concordance line, given by r, is *not* the same as the slope given by a fitted line. The slope of the *concordance line* shows the *agreement* between variables (i.e., how much a given observed value differs from the expected value). The slope is limited to a max of 1, showing perfect agreement. The *fitted* line shows the actual relationship between U and W, and its slope is not bounded between -1 and 1. 

Consider the following data. U will be compared to W1, W2, and W3, yielding the same concordance line, but different fitted lines. 


```{r , dpi=300, fig.width=8, fig.height=3.5}
options(warn=-1)

data_r = data.frame( #create a data set
  U = c(0:10)) %>% # here are the values from set U
  mutate(W1 = U+10,  # set W, which
         W2 = U*2, 
         W3 = U) %>% print() %>% 
  pivot_longer(cols = -U, values_to = "W_values")

ggplot(data_r, aes(x=U, y= W_values))+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  geom_line()+
  geom_point(aes(x=U, y= W_values))+

  facet_wrap(~name)+
  
  labs(x = "Values of U", y = "Values of W")+
  stat_cor(method = "pearson", p.accuracy = 0.001, cor.coef.name = "r")+
  
   theme(
    panel.background = element_blank() ,
    panel.grid.major = element_line(colour = "grey95") ,
    panel.border     = element_rect(linetype = "solid",colour = "grey80", linewidth=0.3, fill = NA),
    
    axis.text  = element_text(colour = "black"),
    plot.title =  element_text(hjust =0, face = "bold", size = rel(1.2)),
    
    strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
    strip.background =  element_blank())


```

In the left-most plot, W1 = U + 10. The slopes of the fitted and concordance lines are identical, but the lines are not. 

In the middle plot, W2 = 2 * U, yielding a fitted line with a steeper slope than the concordance line. 

In the right-most plot, W3 = U. In this case, the fitted line *is* identical to the concordance line. 

In all three cases, the fitted relationship between U and W changed. The concordance line didn't. We can also see that, despite the fact that very different ratings were considered (W1, W2, and W3), the concordance line still shows perfect agreement between observed and expected values. 

The second term in the simplied $\hat{\rho_c}$ equation is **$C_b$**, which quantifies the *distance* or *shift* from the best fitting line to the concordance line. 

$C_b$ is calculated using the following equation: 

$$C_b = 2/(\nu + 1/\nu + \mu^2)$$
where $\nu = \sigma_U/\sigma_W$ and $\mu = (\mu_U - \mu_W)/\sqrt{\sigma_U*\sigma_W}$.

$\mu$ quantifies the **location shift**, or the difference in the *y-axis intercept* between the fitted line and the concordance line. In this type of shift, the value between U and W differ by a constant amount. An example of a location shift can be found in the previous charts, where U is compared to W1. 

$\nu$ represents the **scale shift**, or the difference in slopes between the fitted and concordance lines. The value of the concordance line may stay at 1 to indicate perfect positive association, while the slope of the fitted line may increase or decrease to represent a relationship where the rate of change is not a constant. 


## **Precision**

Precision can be calculated between a single assessor's rating results (**intra-rater**), or between the results of multiple assessors (**inter-rater**). The type of calculation you perform depends on the number of people rating plant specimens, the time span of the evaluation, and many more factors. 

Intra-rater precision measures determine if the same person produces similar results in repeat or similar evaluations. A question that could be answered by evaluating intra-rater precision is "Are this person's ratings consistent across different time periods, or in different circumstances?". 

For plant disease measurements, precision can be subdivided into **intra-rater** and **inter-rater** reliability, where **intra-rater** refers to precision by examining if the *same rater* can produce similar results when measuring the same specimens on *different assessments*. **Inter-rater** reliability refers to the concept where  *different raters* can produce similar results when measuring the same specimens. These concepts are also known as intra-rater variability, and intra-rater repeatability, or just repeatibility (Nutter 2002)

Below, code is provided to load data into R, the output shows the first six rows of a data set described in the introduction. The column `set` is the set of data from the first (`Set 1`) or the second read (`Set 2`) of data; `leaf_id` is the individual leaf id number; `order` is the order where the leaf was showed to the raters; `truth` is the real value of severity; `raters` is the raters id; `value` is the severity value assigned by the rater; `dev` is the difference between the true value and the value measured by the rate. Depending on your operating system, you will need to modify the read line accordingly. 


```{r setup, include=TRUE, message = FALSE, warning = FALSE}

data_rate = readxl::read_xlsx("data/severity_rates.xlsx") %>%
  pivot_longer(cols = -c("set", "leaf_id", "order", "truth"), 
               names_to = "raters") %>% 
  group_by(set, leaf_id) %>% 
  mutate(dev = value-truth,
         set = factor(set, levels = c(1, 2), labels = c("Set 1", "Set 2")))
  

head(data_rate)
```

<br>

The plot below shows the distribution for each rater's rating by set. The true values are provided on the left axis as marks. The true values ranged from 11 to 68% (mean = 41.4%; median = 45.5%), while the range of estimated values across raters was 2 (R2) to 90% (R5, R6, R7, and R9) for Set 1, and 3 (R2 and R9) to 95% (R6) for Set 2. The mean and median across all raters were 43.5% and 41%, and 39.1 and 35%, for Set 1 and Set 2, respectively.

<br>

```{r BOXPLOT_rate, dpi=300, fig.width=8, fig.height=4.5}
# Readings per rater
ggplot(data_rate)+
  geom_boxplot(outlier.alpha = 0, aes(x = raters, y = value))+
  geom_jitter(aes(fill = raters, x = raters, y = value), width = 0.2,
              show.legend = FALSE, shape = 21, color = "black", size = 2)+
  geom_rug(aes(y = truth))+
  
  ggsci::scale_fill_npg()+
  scale_y_continuous(limits = c(0,100))+
  scale_x_discrete(expand =  c(.1,0))+
  
  labs(y = "Severity (%)", x = NULL, 
       title = "Disease severity (%) distribution per rater and set",
       subtitle = "Each point represents a leaf reading \nRugs in the plot left are the true values")+
  
  facet_wrap(~set)+
  
  theme(
    panel.background = element_blank() ,
    panel.grid.major = element_line(colour = "grey95") ,
    panel.border     = element_rect(linetype = "solid",colour = "grey80", size=0.3, fill = NA),
    
    axis.text  = element_text(colour = "black"),
    plot.title =  element_text(hjust =0, face = "bold", size = rel(1.2)),
    
    strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
    strip.background =  element_blank())
```

<br>
<br>

### **Intra-rater reliability**

The plot below shows the relationship between the ratings for the first and the second sets. While each set is identical, meaning that they have the same leaves, the order was different between the first and the second ratings (in this example, a short break was provided to participants between ratings). A rater showing perfect *intra-rater reliability* will have all the points fall on the dashed-line (concordance line). The solid line is the linear regression between set 1 and set 2, indicating if there is a trend of a rater being more (or less) precise depending on the disease intensity.


```{r Intra-rater reliability, fig.height=8, fig.width=8.5, message=FALSE, warning=FALSE, dpi=300}
 data_intra_rate =  data_rate %>% ungroup() %>% 
    select(set, leaf_id, truth, raters, value) %>% 
    pivot_wider(names_from = set, values_from = value) %>% 
    rename(set1 = `Set 1`, set2 = `Set 2`)
 
 ggplot(data_intra_rate) +
   geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
   geom_smooth(aes(x=set1, y = set2, color = raters),
               show.legend = FALSE, method="lm", se = FALSE)+  
   
   geom_point(aes(x=set1, y = set2, fill = raters), size = 3, 
              show.legend = FALSE, shape = 21, color = "black")+

   ggsci::scale_color_npg()+
   ggsci::scale_fill_npg()+
   scale_x_continuous(limits = c(0,100))+
   scale_y_continuous(limits = c(0,100))+  
   
   labs(x = "Ratings from the first set", 
        y = "Ratings from the second set", 
        title = "Relationship between dissease assessment from Set 1 and Set 2", 
        subtitle = "Dashed line is the line or perfect agreement, solid line is the linear regression")+
   
   facet_wrap(~raters, ncol = 3)+
   theme( panel.background = element_blank() ,
          panel.grid.major = element_line(colour = "grey95") ,
          panel.border     = element_rect(linetype = "solid",colour = "grey80", size=0.3, fill = NA),
     
          axis.text  = element_text(colour = "black"),
          plot.title =  element_text(hjust =0, face = "bold", size = rel(1.2)),
         
          strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
          strip.text.y =  element_text(face = "bold", size = rel(1.5)),
          strip.background =  element_blank())
 
```


<br>
<br>
The statistics of precision and accuracy shows that most of the raters were relatively consistent from  set 1 to set 2 (=intra-rate reliability), with 6 of the 9 having $\hat{\rho_c}$ above 0.85. The rater with the lowest value of $\hat{\rho_c}$ was R6, with $\hat{\rho_c}$ = 0.7. A closer look for the results for R6 and we can see that their problem was not the accuracy ($C_b$ = 0.93), but rather the precision, with the *r* across all raters (*r* = 0.75). Rater R9 had the second lowest value of $\hat{\rho_c}$ (0.73), different than R6, their problem was more associated with accuracy ($C_b$ = 0.85). A closer look at the components of $C_b$ and we can identify that the $\mu$ was relative far from zero ($\mu$ = 0.59), and this is easily spotted in the graph. Although the values for lower and higher values given for the Set 1 are similar to Set 2, intermediary values (from 25 to 75) given on Set 1, were underrated in the Set 2.


```{r intra-rater stats}
table_intra_rater = 
 data_intra_rate %>% 
   group_by(raters) %>% 
   summarise( set_m1 = mean(set1),
              sd1 = sd(set1),
              set_m2 = mean(set2),
              sd2 = sd(set2),
              r   = cor(set1, set2) ) %>%
mutate(v  = (sd1/sd2),
          u  = (set_m1-set_m2)/sqrt(sd1*sd2), 
          cb = 2/((v+(1/v)+u**2)),
          pc = r*cb) %>% 
   mutate_at(c("set_m1", "sd1", "set_m2", "sd2"), round, digits=1) %>% 
   mutate_at(c("r", "v", "u", "cb", "pc"), round, digits=2)

 kable(table_intra_rater, align = "lcccccccc", 
       col.names = c("Rater", "Mean Set1", "SD Set1", "Mean Set2", "SD Set2", "*r*", 
                      "$\\nu$", "$\\mu$", "$C_b$", "$\\hat{\\rho_c}$")) %>% 
 kable_styling("striped")  
 
```


<br>
<br>


### **Inter-rater reliability**

<br>
<br>
To avoid redundancy, for this section on inter-rater reliability, we will use R2 as the standard comparison with the others eight raters, since they obtained the highest value of $\hat{\rho_c}$, 0.91. We also are only using the measurements from the Set 1. What you will see below are the graphical summaries of the relationship between the observations, along with the table of statistical results.


```{r Inter-rate reliability, fig.height=12.5, fig.width=8, message=FALSE, warning=FALSE, dpi=300}

data_inter_rate = 
data_rate %>% 
  ungroup() %>% 
  select(-dev, -order) %>% 
  filter(set =="Set 1") %>% 
  pivot_wider(names_from = raters, values_from = value) %>% 
  select(-set, -leaf_id, - truth) %>% 
  pivot_longer(-R2, names_to = "raters") %>% 
  mutate(comp = paste(raters, "vs R2"))

  ggplot(data_inter_rate) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  geom_smooth(aes(x=R2, y = value, color = comp),
              show.legend = FALSE, method="lm", se = FALSE)+  
  
  geom_point(aes(x=R2, y = value, fill = comp), size = 3, 
             show.legend = FALSE, shape = 21, color = "black")+
  
  ggsci::scale_color_npg()+
  ggsci::scale_fill_npg()+
  scale_x_continuous(limits = c(0,100))+
  scale_y_continuous(limits = c(0,100))+  
  
  labs(y = "Disease severity (%)", x = "Disease severity (%) rating by R2", 
       title = "Relationship between disease severity ratings from 8 raters and R2",
       subtitle = "Rater R2 showed the best intra-rater reliability")+
    
  facet_wrap(~comp, ncol = 2)+
  theme( panel.background = element_blank() ,
         panel.grid.major = element_line(colour = "grey95") ,
         panel.border     = element_rect(linetype = "solid",colour = "grey80", size=0.3, fill = NA),
         
        plot.title =  element_text(hjust =0, face = "bold", size = rel(1.2)),
         axis.text  = element_text(colour = "black"),
         
         strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
         strip.text.y =  element_text(face = "bold", size = rel(1.5)),
         strip.background =  element_blank())

```



```{r inter-rater stats}

  data_inter_rate %>% 
    group_by(raters) %>% 
    summarise( set_m1 = mean(value),
               sd1 = sd(value),
               set_m2 = mean(R2),
               sd2 = sd(R2),
               r   = cor(value, R2) ) %>%
    mutate(v  = (sd1/sd2),
           u  = (set_m1-set_m2)/sqrt(sd1*sd2), 
           cb = 2/((v+(1/v)+u**2)),
           pc = r*cb) %>% 
    mutate_at(c("set_m1", "sd1", "set_m2", "sd2"), round, digits=1) %>% 
    mutate_at(c("r", "v", "u", "cb", "pc"), round, digits=2) %>% 
    select(-set_m2, -sd2) %>% 
  
  kable(., align = "lcccccc", 
        col.names = c("Rater", "Mean Raters", "SD Raters", "*r*", 
                      "$\\nu$", "$\\mu$", "$C_b$", "$\\hat{\\rho_c}$")) %>% 
    kable_styling("striped")
 
```

R1, R3, and R9 were the only raters that produced $\hat{\rho_c}$ values >0.8. R4, R5, R6, and R7 had relatively poor performance with $\hat{\rho_c}$ values <0.6. The majority of significant differences were observed in the $C_b$ parameter, although, R1, R3, and R4 also had better performance based on *r*. $C_b$ values were $\le 0.7$ on the four raters with the worst performance. Three of these raters (R5, R6, and R7) had $\nu \ge 1.0$, indicating that there was a large scale shift between these raters and R2. R8 were the only rater with $\mu$ close to 0. R4 was the rater with the highest location shift, followed by R6, with $\mu$ equals to 0.78 and 1.37, respectively.

## **Accuracy**

Now, the final comparison is between the true values and the estimated value. For this analysis we use only data from Set 1. Nonetheless, the box plot below shows how each observation, from Set 1 and Set 2, differs from the true value. 

```{r error from real, dpi=300, fig.width=8, fig.height=4}

ggplot(data_rate, aes(x = raters, y = dev))+
  geom_boxplot(outlier.alpha = 0)+
  geom_jitter(aes(fill = raters), width = 0.2,
              show.legend = FALSE, shape = 21, color = "black", size = 2)+  
  
  ggsci::scale_fill_npg()+
  scale_y_continuous(limits = c(-50,50))+

  facet_wrap(~set)+
  
  labs(x = "Raters", y = "Deviation from true value")+
  
  theme(
    panel.background = element_blank() ,
    panel.grid.major = element_line(colour = "grey95") ,
    panel.border     = element_rect(linetype = "solid",colour = "grey80", size=0.3, fill = NA),
    
    axis.text  = element_text(colour = "black"),
    
    strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
    strip.background =  element_blank())

```

Below, the graph and table with the statistical analysis for accuracy (observed vs true value).

```{r Accuracy, fig.height=8, fig.width=8.5, message=FALSE, warning=FALSE, dpi=300}


acc_data =
  data_rate %>%  ungroup() %>% 
    filter(set =="Set 1") %>% 
    select(truth, raters, value) %>% 
     group_by(raters)

ggplot(acc_data) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  geom_smooth(aes(x=truth, y = value, color = raters),
              show.legend = FALSE, method="lm", se = FALSE)+  
  
  geom_point(aes(x=truth , y = value, fill = raters), size = 3, 
             show.legend = FALSE, shape = 21, color = "black")+
  
  ggsci::scale_color_npg()+
  ggsci::scale_fill_npg()+
  scale_x_continuous(limits = c(0,100))+
  scale_y_continuous(limits = c(0,100))+  
  
  labs(x = "Disease severity (%) true value", 
       y = "Rates estimate by raters")+
  
  facet_wrap(~raters, ncol = 3)+
  theme( panel.background = element_blank() ,
         panel.grid.major = element_line(colour = "grey95") ,
         panel.border     = element_rect(linetype = "solid",colour = "grey80", size=0.3, fill = NA),
         
         axis.text  = element_text(colour = "black"),
         
         strip.text.x =  element_text(hjust =0, face = "bold", size = rel(1.5)),
         strip.text.y =  element_text(face = "bold", size = rel(1.5)),
         strip.background =  element_blank())

```


```{r}
acc_data %>% 
  group_by(raters) %>% 
  summarise( set_m1 = mean(value),
             sd1 = sd(value),
             set_m2 = mean(truth),
             sd2 = sd(truth),
             r   = cor(value, truth)) %>%
  mutate(v  = (sd1/sd2),
         u  = (set_m1-set_m2)/sqrt(sd1*sd2), 
         cb = 2/((v+(1/v)+u**2)),
         pc = r*cb) %>% 
  mutate_at(c("set_m1", "sd1", "set_m2", "sd2"), round, digits=1) %>% 
  mutate_at(c("r", "v", "u", "cb", "pc"), round, digits=2)%>% 
  select(-set_m2, -sd2) %>% 
  
  kable(., align = "lcccccc", 
        col.names = c("Rater", "Mean Raters", "SD Raters", "*r*", 
                      "$\\nu$", "$\\mu$", "$C_b$", "$\\hat{\\rho_c}$")) %>% 
  kable_styling("striped")


```


In comparison with the intra- and inter-rate reliability, values of $\hat{\rho_c}$ were overall lower, with none of the raters having $\hat{\rho_c} \ge 0.9$ and one (R4) with $\hat{\rho_c} = 0.4$. R4 had the lowest *r* and $C_b$, which indicates both poor precision and accuracy. Raters R1, R2, and R3 were the only ones with $\hat{\rho_c} > 0.8$. Considering the scale shift, all, but R1, R3 and R9, have $\nu > 0.35$, with R4 having a very strong scale location ($\nu = -1.4$). Except by R4 and R8, all raters showed some location shift, with $\mu > 1.35$.

<!-- **# Extra** -->
<!-- **Other methods (?)** -->
<!-- **Confusion matrix (?)** -->

# **References**

Bock CH, Poole GH, Parker., PE, Gottwald TR (2010) Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging. [Critical Reviews in Plant Sciences 29:59–107.](https://doi.org/10.1080/07352681003617285)

Bock CH, Barbedo JGA, Del Ponte EM, Bohnenkamp D, Mahlein A-K (2020) From visual estimates to fully automated sensor-based measurements of plant disease severity: status and challenges for improving accuracy. [Phytopathol. Res. 2:1–36.](https://doi.org/10.1186/s42483-020-00049-8)

Del Ponte EM, Pethybridge SJ, Bock CH, Michereff SJ, Machado FJ, Spolti P (2017) Standard area diagrams for aiding severity estimation: scientometrics, pathosystems, and methodological trends in the last 25 years. [Phytopathology 107:1161–1174.](https://doi.org/10.1094/PHYTO-02-17-0069-FI)

Madden LV, Hughes G, van den Bosch, F (2007) The Study of Plant Disease Epidemics. APS Press, St. Paul, MN, USA.

Mahlein A-K (2016) Plant Disease detection by imaging sensors - Parallels and specific demands for precision agriculture and plant phenotyping. [Plant Dis. 100:241–251.](https://doi.org/10.1094/PHYTO-04-15-0100-R)

Nutter FW Jr, Esker PD, Coelho Netto RA (2006) Disease assessment concepts in plant pathology. [European Journal of Plant Pathology 115:95–103.](https://doi.org/10.1007/s10658-005-1230-z)
